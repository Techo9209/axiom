books/bookvolbib add references

Goal: Proving Axiom Sane

\index{Driscoll, Kevin R.}
\begin{chunk}{axiom.bib}
@misc{Dris20,
  author = "Driscoll, Kevin R.",
  title = {{Murphy Was An Optimiist}},
  year = "2020",
  link = "\url{www.rvs.uni-bielefeld.de/publications/DriscollMurphyv19.pdf}",
  comment = "Kevin.Driscoll@Honeywell.com",
  paper = "Dris20.pdf"
}

\end{chunk}

\index{Doctorow, Cory}
\begin{chunk}{axiom.bib}
@misc{Doct11,
  author = "Doctorow, Cory",
  title = {{The Coming War on General Computation}},
  year = "2011",
  link = "\url{http://opentranscripts.org/transcript/coming-war-on-general-computation}",
  keywords = "DONE"
}

\end{chunk}

\index{Farvardin, Kavon}
\index{Reppy, John}
\begin{chunk}{axiom.bib}
@inproceedings{Farv20,
  author = "Farvardin, Kavon",
  title = {{From Folklore to Fact: Comparing Implementation of Stacks
            and Continuations}},
  booktitle = "Programming Language Design and Implementation",
  publisher = "ACM SIGPLAN",
  year = "2020",
  abstract =
    "The efficient implementation of function calls and non-local
    control transfers is a critical part of modern language
    implementations and is important in the implementation of
    everything from recursion, higher-order functions, concurrency and
    coroutines, to task-based parallelism. In a compiler, these
    features can be supported by a variety of mechanisms, including
    call stacks, segmented stacks, and heap-allocated continuation
    closures. 

    An implementor of a high-level language with advanced control
    features might ask the question ``what is the best choice for my
    implementation?'' Unfortunately, the current literature does not
    provide much guidance, since previous studies suffer from various
    flaws in methodology and are outdated for modern hardware. In the
    absence of recent, well-normalized measurements and a holistic
    overview of their implementation specifics, the path of least
    resistance when choosing a strategy is to trust folklore, but the
    folklore is also suspect.

    This paper attempts to rememdy this situation by providing an
    ``apples-to-apples'' comparison of five different approaches to
    implementing call stacks and continuations. This comparison uses
    the same source language, compiler pipeline, LLVM-backend, and
    runtime system, with the only differences being those required by
    the differences in implementation strategy. We compare the
    implementation challenges of the different approaches, their
    sequential performance, and their suitability to support advanced
    control mechanisms, including supporting heavily threaded code. In
    addition to the comparison of implementation strategies, the
    paper's contributions also include a number of useful
    implementation techniques that we discover along the way.",
  paper = "Farv20.pdf"
}

\end{chunk}

\index{Srivastava, Saurabh}
\index{Gulwani, Sumit}
\index{Foster, Jeffrey S.}
\begin{chunk}{axiom.bib}
@inproceedings{Sriv10,
  author = "Srivastava, Saurabh and Gulwani, Sumit and Foster, Jeffrey S.",
  title = {{From Program Verification to Program Synthesis}},
  booktitle = "37th Symp. on Principles of Programming Languages",
  publisher = "ACM",
  pages = "313-326",
  year = "2010",
  abstract =
    "This paper describes a novel technique for the synthesis of
    imperative programs. Automated program synthesis has the potential
    to make programming and the design of systems easier by allowing
    programs to be specified at a higher-level than executable
    code. In our approach, which we call proof-theoretic synthesis,
    the user provides an input-output functional specification, a
    description of the atomic operations in the progamming language,
    and a specification of the synthesized program's looping
    structure, allowed stack space, and bound on usage of certain
    operations. Our technique synthesizes a program, if there exists
    one, that meets the input-output specification and uses only the
    given resources.

    The insight behind our approach is to interpret program synthesis
    as generalized program verification, which allows us to bring
    verification tools and techniques to program synthesis. Our
    synthesis algorithm works by creating a program with unknown
    statements, guards, inductive invariants, and ranking
    functions. It then generates constraints that relate the unknowns
    and enforces three kinds of requirements: partial correctness,
    loop termination, and well-formedness conditions on program
    guards. We formalize the requirements that program verification
    tools must meet to solve these constraints and use tools from
    prior work as our synthesizers.

    We demonstrate the feasibility of the proposed approach by
    synthesizing programs in three different domains: arithmetic,
    sorting, and dynamic programming. Using verification tools that we
    previously built in the VS3 project we are able to synthesize
    programs for complicated arithmetic algorithms including
    Strassen's matrix multiplication and Bresenham's line drawing;
    several sorting algorithms; and several dynamic programming
    algorithms. For these programs, the median time for synthesis is
    14 seconds, and the ratio of synthesis to verification time ranges
    between 1x to 92x (with a median of 7x), illustrating the potential
    of the approach.",
  paper = "Sriv10.pdf"
}

\end{chunk}

\index{Hoare, Tony}
\begin{chunk}{axiom.bib}
@article{Hoar95,
  author = "Hoare, Tony",
  title = {{Unification of Theories: A Challenge for Computing Science}},
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer",
  volume = "1130",
  year = "1995",
  abstract =
    "Unification of theories is the long-standing goal of the natural
    sciences; and modern physics offers a spectacular paradigm of its
    achievement. The structure of modern mathematics has also been
    determined by its great unifying theories -- topology, algebra and
    the like. The same ideals and goals are shared by researchers and
    students of theorectical computing science.",
  paper = "Hoar95.pdf"
}

\end{chunk}

\index{Mylonakis, Nikos}
\begin{chunk}{axiom.bib}
@article{Mylo95,
  author = "Mylonakis, Nikos",
  title = "Behavioural Specifications in Type Theory",
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer",
  volume = "1130",
  year = "1995",
  abstract =
    "In this paper we give a new view of the type theory UTT (Uniform
    theory of dependent types) [5] as a system to formally develop
    programs from algebraic specifications, comparable to
    e.g. EML([9]). We will focus our attention on behavioural
    specifications since they have not been deeply studied in a type
    theoretical setting, and we describe how to develop proofs about
    behavioural satifaction.",
  paper = "Mylo95.pdf"
}

\end{chunk}

\index{Shparlinski, Igor E.}
\begin{chunk}{axiom.bib}
@book{Shpa99,
  author = "Shparlinski, Igor E.",
  title = {{Finite Fields: Theory and Computation}},
  publisher = "Kluwer Academic",
  year = "1999",
  isbn = "0-7923-5662-4",
  keywords = "axiomref"
}

\end{chunk}

\index{Lescanne, Pierre}
\begin{chunk}{axiom.bib}
@article{Lesc95,
  author = "Lescanne, Pierre",
  title = {{The Lambda Calculus as an Abstract Data Type}},
  booktitle = "Lecture Notes in Computer Science",
  publisher = "Springer",
  volume = "1130",
  year = "1995",
  abstract =
    "Calculi of explicit substitutions are theoretical tools to
    describe lambda calculus as first-order rewrite systems. Therefore
    a presentation as an abstract data type is possible. During an
    invited lecture at the Workshop on Abstract Data Types, we
    presented calculi of explicit substitutions, especially the
    calculus $\lambda$v. The theoretical background of $\lambda$v can
    be found in [1], therefore we are not entering in the details of
    the theory as we did during the oral presentation. However, we
    take the essence of the talk, namely the formalizatino of
    $\lambda$v in a specification language based on ADT. We have
    chosen LSL, the LARCH Shared Language [7] to illustrate the beauty
    of abstract data types which lies in its precision and
    concision. Moreoever we take freedom with respect to this
    language.",
  paper = "Lesc95.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Lamport, Leslie}
\begin{chunk}{axiom.bib}
@misc{Lamp09,
  author = "Lamport, Leslie",
  title = {{The PlusCal Algorithm Language}},
  year = "2009",
  link = "\url{https://lamport.azurewebsites.net/pubs/pluscal.pdf}",
  abstract =
    "Algorithms are different from programs and should not be
    described with programming languages. The only simple alternative
    to programming languages has been pseudo-code. PlusCal is an
    algorithm language that an be used right now to replace
    pseudo-code, for both sequential and concurrent algorithms. It is
    based on the TLA+ specification language, and a PlusCal algorithm
    is automatically translated to a TLA+ specification that can be
    checked with the TLC model checker and reasoned about formally.",
  paper = "Lamp09.pdf",
  keywords = "printed"
}  

\end{chunk}

\index{Sedgewick, Robert}
\begin{chunk}{axiom.bib}
@book{Sedg88,
  author = "Sedgewick, Robert",
  title = {{Algorithms}},
  publisher = "Addison-Wesley",
  year = "1988"
}

\end{chunk}

\index{Armstrong, Joe}
\begin{chunk}{axiom.bib}
@misc{Arms14,
  author = "Armstrong, Joe",
  title = {{The Mess We're In}},
  link = "\url{https://www.youtube.com/watch?v=lKXe3HUG2l4}",
  year = "2014",
  keywords = "axiomref, DONE"
}

\end{chunk}

\index{Turing, A. M.}
\begin{chunk}{axiom.bib}
@article{Turi49,
  author = "Turing, A. M.",
  title = {{Checking a Large Routine}},
  journal = "Annals of the History of Computing",
  volume = "6",
  number = "2",
  year = "1949",
  abstract = 
    "The standard references for work on program proofs attribute the
    early statement of direction to John McCarthy (e.g. McCarthy
    1963); the first workable methods to Peter Naur (1966) and Robert
    Floyd (1967); and the provision of more formal systems to C.A.R
    Hoare (1969) and Edsger Dijkstra (1976). The early papers of some
    of the computing pioneers, however, show an awareness of the need
    for proofs of program correctness and even present workabe methods
    (.e.g Goldstine and von Neumann 1947; Turing 1949).

    The 1949 paper by Alan M. Turing is remarkable in many
    respects. The three (foolscap) pages of text contain an excellent
    motivation by analogy, a proof of a program with two nested loops,
    and an indication of a general proof method very like that of
    Floyd. Unfortunately, the paper is made extremely difficult to
    read by a large number of transcription errors. For example, all
    instances of the factorial sign (Turing used $|n$) have been
    omitted in the commentary, and ten other identifiers are written
    incorrectly. It would appear to be worth correcting these errors
    and commenting on the proof from the viewpoint of subsequent work
    on program proofs.

    Turing delivered this paper in June 1949, at the inaugural
    conference of the EDSAC, the computer at Cambridge University
    built under the direction of Maurice V. Wilkes. Turing had been
    writing programs for an electronic computer since the end of 1945
    -- at first for the proposed ACE, the computer project at the
    National Physical Laboraty, but since October 1948 for the
    Manchester prototype computer, of which he was deputy
    director. The references in his paper to $2^{40}$ are reflections
    of the 40-bit ``lines'' of the Manchester machine storage system.

    The following is the text of Turing's 1949 paper, corrected to the
    best of our ability to what we believe Turing intended. We have
    made no changes in spelling, punctuation, or grammar.",
  paper = "Turi49.pdf"
}

\end{chunk}

\index{Hoare, Charles Antony Richard}
\begin{chunk}{axiom.bib}
@article{Hoar81,
  author = "Hoare, Charles Antony Richard",
  title = {{The Emperor's Old Clothes}},
  journal = "CACM",
  volume = "24",
  number = "2",
  pages = "75-83",
  abstract =
    "The author recounts his experiences in the implementation,
    design, and standardization of computer programming languages, and
    issues a warning for the future.",
  paper = "Hoar18.pdf",
  keywords = "DONE"
}

\end{chunk}

\index{Wirth, Niklaus}
\begin{chunk}{axiom.bib}
@article{Wirt95,
  author = "Wirth, Niklaus",
  title = {{A Plea for Lean Software}},
  publisher = "IEEE",
  journal = "Computer",
  year = "1995",
  pages = "64-68",
  paper = "Wirt95.pdf",
  keywords = "DONE"
}

\end{chunk}

\index{Liskov, Barbara}
\begin{chunk}{axiom.bib}
@misc{Lisk12,
  author = "Liskov, Barbara",
  title = {{Programming the Turing Machine}},
  year = "2012",
  link = "\url{https://www.youtube.com/watch?v=ibRar7sWulM}",
  keywords = "DONE"
}

\end{chunk}

\index{Di Franco, Anthony}
\index{Rubio-Gonzalez, Cindy}
\begin{chunk}{axiom.bib}
@misc{Difr17,
  author = "Di Franco, Anthony and Rubio-Gonzalez, Cindy",
  title = {{A Comprehensive Study of Real-World Numerical Bug
            Characteristics}}, 
  year = "2017",
  link = "\url{https://web.cs.ucdavis.edu/~rubio/includes/ase17.pdf}",
  abstract = 
    "Numerical software is used in a wide variety of applications
    including safety-critical systems, which have stringent
    correctness requirements, and whose failures have catastrophic
    consequences that endanger human life. Numerical bugs are known to
    be particularly difficult to diagnose and fix, largely due to the
    use of approximate representations of numbers such as floating
    point. Understanding the characteristics of numerical bugs is the
    first step to combat them more effectively. In this paper, we
    present the first comprehansive study of real-world numerical
    bugs. Specifically, we identify and carefully examine 269
    numerical bugs from five widely-used numerical software libraries:
    NumPy, SciPy, LAPACK, GNU Scientific Library, and Elemental. We
    propose a categorization of numerical bugs, and discuss their
    frequency, symptoms and fixes. Our study opens new directions in
    the areas of program analysis, testing, and automated program
    repair of numerical software, and provides a collection of
    real-world numerical bugs.",
  paper = "Difr17.pdf"
}    

\end{chunk}

\index{Talcott, Carolyn}
\begin{chunk}{axiom.bib}
@inproceedings{Talc90,
  author = "Talcott, Carolyn",
  title = {{A Theory for Program and Data Type Specifications}},
  booktitle = "DISCO 1990",
  year = "1990",
  pages = "91-100",
  paper = "Talc90.pdf"
}

\end{chunk}

\index{Jouannaud, Jean-Pierre}
\index{Marche, Claude}
\begin{chunk}{axiom.bib}
@inproceedings{Joua90,
  author = "Jouannaud, Jean-Pierre",
  title = {{Completion modulo Associativity, Commutativity and
            Identity (ACI)}},
  booktitle = "DISCO 1990",
  year = "1990",
  pages = "111-120",
  abstract =
    "Rewriting with associativity, commutativity and identity has been
    an open problem for along time. In a recent paper [PBW89], Baird,
    Peterson and Wilkerson introduced the notion of constrained
    rewriting, to avoid the problem of non-termination inherent to the
    use of identities. We build up on this idea in two ways: by giving
    a complete set of rules for completion modulo these axioms; by
    showing how to build appropriate orderings for proving termination
    of constrained rewriting modulo associativity, commutativity and
    identity.", 
  paper = "Joua90.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Reynaud, Jean-Claude}
\begin{chunk}{axiom.bib}
@inproceedings{Reyn90,
  author = "Reynaud, Jean-Claude",
  title = {{Putting Algebraic Components Together: A Dependent Type
            Approach }},
  booktitle = "DISCO 1990",
  year = "1990",
  pages = "141-150",
  abstract =
    "We define a framework based on dependent types for putting
    algebraic components together. It is defined with freely generated
    categories. In order to preserve initial, loos and constrainted
    semantics of components, we introduce the notion of
    SPEC-categories which look like specific finitely co-complete
    categories. A constructive approach which includes
    parameterization techniques is sued to define new components from
    basic predefined ones. The problem of the internal coding of
    external signature symbols is introduced.",
  paper = "Reyn90.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Dewar, M.C.}
\index{Richardson, M.G.}
\begin{chunk}{axiom.bib}
@inproceedings{Dewa90,
  author = "Dewar, M.C. and Richardson, M.G.",
  title = {{Reconciling Symbolic and Numeric Computation in a
            Practical Setting}},
  booktitle = "DISCO 1990",
  year = "1990",
  pages = "195-204",
  paper = "Dewa90.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Jebelean, Tudor}
\begin{chunk}{axiom.bib}
@inproceedings{Jebe93,
  author = "Jebelean, Tudor",
  title = {{Improving the Multiprecision Euclidean Algorithm}},
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "45-58",
  publisher = "Springer",
  abstract =
    "We improve the implementation of Lehmer-Euclid algorithm for
    multiprecision integer GCD computations by partial consequence
    computation on pairs of double digits, enhanced conditions for
    exiting the partia consequence computatin, and approximative GCD
    computation. The combined effect of these improvements is an
    experimentally measured speed-up by a factor of 2 over the
    currently used implementation.",
  paper = "Jebe93.pdf",
  keywords = "printed"
}  

\end{chunk}

\index{Hennicker, Rolf}
\begin{chunk}{axiom.bib}
@inproceedings{Henn90,
  author = "Hennicker, Rolf",
  title = {{Context Induction: A Proof Principle for Behavioural
            Abstractions}}, 
  booktitle = "DISCO 1990",
  year = "1990",
  pages = "101-110",
  abstract = 
    "An induction principle, called context induction, is presented
    which is appropriate for the verification of behavioural
    properties of abstract data types. The usefulness of the proof
    principle is documented by several applications: the verification
    of behavioural theorems over a behavioural specification, the
    verification of behavioural implementations and the verification
    of ``forget=restrict-identify" implementations.

    In particular it is shown that behavioural implementations and
    ``forget-restrict-identify'' implementations (under certain
    assumptions) can be characterized by the same context condition,
    i.e. (under the given assumptions) both concepts are
    equivalent. This leads to the suggestion to use context induction
    as a uniform proof method for correctness proofs of formal
    implementations.",
  paper = "Henn90.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Calmet, J.}
\index{Tjandra, I.A.}
\begin{chunk}{axiom.bib}
@inproceedings{Calm93,
  author = "Calmet, J. and Tjandra, I.A.",
  title = {{A Unified-Algebra-based Specification Language for
            Symbolic Computing}},
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "122-133",
  publisher = "Springer",
  abstract =
    "A precise and perspicuous specification of mathematical domains
    of computation and their inherently related type inference
    mechanisms is a prerequisite for the design and systematic
    development of a system for symbolic computing. This paper
    describes FORMAL, a language for giving modular and
    well-structured specifications of such domains and particularly of
    ``mathematical objects''. A novel framework for algebraic
    specification involving so-called ``unified algebras'' has been
    adopted, where sorts are treated as values. The adoption of this
    framework aims also at being capable of specifying polymorphism,
    unifying the notions of ``parametric'' and ``inclusion''
    polymorphisms. Furthermore, the operational nature of the
    specification formalisms allows a straightforward transformation
    into an executable form.",
  paper = "Calm93.pdf",
  keywords = "printed, axiomref"
}  

\end{chunk}

\index{Fritzson, Peter}
\index{Engelson, Vadim}
\index{Viklund, Lars}
\begin{chunk}{axiom.bib}
@inproceedings{Frit93,
  author = "Fritzson, Peter and Engelson, Vadim and Viklund, Lars",
  title = {{Variant Handling, Inheritance and Composition in the
            ObjectMath Computer Algebra Environment}},
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "145-163",
  publisher = "Springer",
  abstract =
    "ObjectMath is a high-level programming environment and modeling
    language for scientific computing which supports variants and
    graphical browsing in the environment and integrates
    object-oriented constructs such as classes and single and multiple
    inheritance within a computer algebra language. In addition,
    composition of objects using the part-of relation and support for
    solution of systems of equations is provided. This environment is
    currently being used for industrial applications in scientific
    computing. The ObjectMath environment is designed to handle
    realistic problems. This is achieved by allowing the user to
    specify transformations and simplifications of formulae in the
    model, in order to arrive at a representation which is efficiently
    solvable. When necessary, equations can be transformed to C++ code
    for efficient numerical solution. The re-use of equations through
    inheritance in general reduces models by a factor of two to three,
    compared to a direct representation in the Mathematica computer
    algebra language. Also, we found that multiple inheritance from
    orthogonal classes facilitates re-use and maintenance of
    application models.",
  paper = "Frit93.pdf",
  keywords = "axiomref"
}  

\end{chunk}

\index{Grivas, Georgios}
\index{Maeder, Roman E.}
\begin{chunk}{axiom.bib}
@inproceedings{Griv93,
  author = "Grivas, Georgios and Maeder, Roman E.",
  title = {{Matching and Unification for the Object-Oriented Symbolic
            Computation System}},
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "164-176",
  publisher = "Springer",
  abstract =
    "Term matching has become one of the most important privmitive
    operations for symbolic computation. This paper describes the
    extension of the object-oriented symbolic computation system
    AlgBench with pattern matching and unification facilities. The
    various pattern objects are organized in subclasses of the class
    of the composite expressions. This leads to a clear design and a
    distributed implementation of the pattern matcher in the
    subclasses. New pattern object classes can consequently be added
    easily to the system. Huet's and our simple mark and retract
    algorithm for standard unification as well as Stickel's algorithm
    for associative commutative unification have been implemented in
    an object-oriented style. Unifiers are selected at runtime. We
    extend Mathematica's type-constrained pattern matching by taking
    into account inheritance information from a user-defined hierarchy
    of object types. the argument unification is basically instance
    variable unification. The improvement of the pattern matching
    operation of a rule- and object-based symbolic computation system
    with unification in an object-oriented way seems to be very
    appropriate.", 
  paper = "Griv93.pdf",
  keywords = "axiomref"
}  

\end{chunk}

\index{Santas, Philip S.}
\begin{chunk}{axiom.bib}
@article{Sant93,
  author = "Santas, Philip S.",
  title = {{A Type System for Computer Algebra}},
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "177-191",
  publisher = "Springer",
  abstract =
    "We examine type systems for support of subtypes and categories in
    computer algebra systems. By modeling representation of instances
    in terms of existential types instead of recursive types, we
    obtain not only a simplified model, but we build a basis for
    defining subtyping among algebraic domains. The introduction of
    metaclasses, facilitates the task, by allowing the inference of
    type classes. By means of type classes and existential types we
    construct subtype relations without involving coercions.",
  paper = "Sant93.pdf",
  keywords = "axiomref"
}  

\end{chunk}

\index{Zwillinger, Daniel}
\begin{chunk}{axiom.bib}
@book{Zwil92,
  author = "Zwillinger, Daniel",
  title = {{Handbook of Integration}},
  publisher = "Jones and Bartlett",
  year = "1992",
  isbn = "0-86720-293-9",
  keywords = "axiomref"
}

\end{chunk}

\index{Limongelli, C.}
\index{Temperini, M.}
\begin{chunk}{axiom.bib}
@inproceedings{Limo93,
  author = "Limongelli, C. and Temperini, M.",
  title = {{On the Uniform Representation of Mathematical Data
            Structures}}, 
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "319-330",
  publisher = "Springer",
  abstract =
    "Topics about the integration of the numeric and symbolic
    computation paradigms are discussed. Mainly an approach through a
    uniform representation of numbers and symbols is presented, that
    allows for the application of algebraic algorithms to numeric
    problems. The $p$-adic constructions is the basis of the unifying
    representation environment. An integrated version of the Hensel
    algorithm is presented, which is able to perform symbolic and
    numeric computations over instances of ground (concrete) and
    parametric structures, and symbolic computations over instances of
    abstract structures. Examples are provided to show how the
    approach outlined and the proposed implementation can treat both
    cases of symbolic and numeric computations. In the numeric case it
    is shown that the proposed extension of the Hensel Algorithm can
    allow for the exact manipulation of numbers. Moreover, such an
    extension avoids the use of simplification algorithms, since the
    computed results are already in simplified form.",
  paper = "Limo93.pdf",
  keywords = "axiomref"
}  

\end{chunk}

\index{Walsh, Toby}
\begin{chunk}{axiom.bib}
@inproceedings{Wals93,
  author = "Walsh, Toby",
  title = {{General Purpose Proof Plans}},
  booktitle = "DISCO 1993",
  year = "1993",
  pages = "319-330",
  publisher = "Springer",
  abstract =
    "One of the key problems in the design and implementation of
    automated reasoning systems is the specification of heuristics to
    control search. ``Proof planning'' has been developed as a
    powerful technique for declaratively specifying high-level proof
    strategies. This paper describes an extension to proof planning to
    allow for the specification of more general purpose plans.",
  paper = "Wals93.pdf"
}

\end{chunk}

\index{Calmet, J.}
\index{Comon, H.}
\index{Lugiez, D.}
\begin{chunk}{axiom.bib}
@article{Calm89,
  author = "Calmet, J. and Comon, H. and Lugiez, D.",
  title = {{Type Inference Using Unification in Computer Algebra}},
  journal = "LNC",
  volume = "307",
  publisher = "Springer",
  paper = "Calm89.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Poll, Erik}
\begin{chunk}{axiom.bib}
@misc{Pollxx,
  author = "Poll, Erik",
  title = {{The type system of Axiom}},
  link = "\url{https://www.cs.ru.nl/E.Poll/talks/axiom.pdf}",
  paper = "Polxx.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Pottier, Francois}
\index{Regis-Gianas, Yann}
\begin{chunk}{axiom.bib}
@inproceedings{Pott06,
  author = "Pottier, Francois and Regis-Gianas, Yann",
  title = {{Stratified Type Inference For Generalized Algebraic Data
            Types}}, 
  booktitle = "POPL'06",
  year = "2006",
  publisher = "ACM",
  abstract =
    "We offer a solution to the type inference problem for an
    extension of Hindley and Milner's type system with generalized
    algebraic data types. Our approach is in two {\sl strata}. The
    bottom stratum is a core language that marries type 
    {\sl inference} in the style of Hindley and Milner with type
    {\sl checking} for generalized algebraic data types. This results
    in an extremely simple specification, where case constructs must
    carry an explicit type annotation and type conversions must be
    made explicit. The top stratum consists of (two variants of) an
    independent {\sl shape inference} algorithm. This algorithm
    accepts a source term that contains some explicit type
    information, propagates this information in a local, predictable
    way, and produces a new source term that carries more explicit
    type information. It can be viewed as a preprocessor that helps
    produce some of the type annotations required by the bottom
    stratum. It is proven {\sl sound} in the sense that it never
    inserts annotations that could contradict the type derivation that
    the programmer has in mind.",
  paper = "Pott06.pdf"
}

\end{chunk}

\index{Calmet, Jacques}
\index{Lugiez, Denis}
\begin{chunk}{axiom.bib}
@article{Calm87a,
  author = "Calmet, Jacques and Lugiez, Denis",
  title = {{A Knowledge-based System for Computer Algebra}},
  journal = "ACM SIGSAM Bulletin",
  volume = "21",
  number = "1",
  year = "1987",
  abstract =
    "This paper reports on a work in progress aiming at designing and
    implementing a system for representing and manipulating
    mathematical knowledge. Its kernel is a computer algebra system
    but it shows several of the features of the so-called
    knowledge-based systems. The main issues considered here are the
    software engineering aspects of the project, the definition of a
    new language to support the system and the use of AI techniques in
    a field where algebraic algorithms are the building stones of
    systems. This defines an environment which enables not only to
    have data-bases of knowledge but also to implement an expert use
    of this knowledge.",
  paper = "Calm87a.pdf",
  keywords = "axiomref"
}

\end{chunk}

\begin{chunk}{axiom.bib}
@misc{HOPLxx,
  author = "unknown",
  title = {{Scratchpad II}},
  year = "2020",
  link = "\url{https://hopl.info/showlanguage2.prx?exp=566}",
  keywords = "axiomref"
}

\end{chunk}

\index{Davenport, James H.}
\begin{chunk}{axiom.bib}
@misc{Dave86a,
  author = "Davenport, James H.",
  title = {{SCRATCHPAD II Programming Language Reference}},
  comment = "IBM T.J. Watson",
  year = "1986"
}

\end{chunk}

\index{Jenks, R.}
\index{Trager, B.}
\index{Watt, S.M.}
\index{Sutor, R.S.}
\begin{chunk}{axiom.bib}
@misc{Jenk85,
  author = "Jenks, R. and Trager, B. and Watt, S.M. and Sutor, R.S.",
  title = {{Scratchpad II Programming Language Manual}},
  year = "1985",
  keywords = "axiomref"
}

\end{chunk}

\index{Chudnovsky, David V.}
\index{Jenks, Richard D.}
\begin{chunk}{axiom.bib}
@book{Chud89,
  author = "Chudnovsky, David V. and Jenks, Richard D.",
  title = {{Computer Algebra. Pure and Applied Mathematics}},
  publisher = "Springer",
  year = "1989"
}

\end{chunk}

\index{Padget, J.A.}
\begin{chunk}{axiom.bib}
@inproceedings{Padg85,
  author = "Padget, J.A.",
  title = {{Current Development in LISP}},
  booktitle = "EUROCAL 85",
  publisher = "Springer",
  year = "1985",
  abstract = 
    "This paper is divided into three sections. The first gives an
    overview of the presently available and emerging dialects of LISP
    and how design decisions within them affect symbolic algebra. The
    second discusses recent developments, in particular, Common LISP
    subsetting, portability, pure language research and mixed paradigm
    systems. The third part is devoted to what is happening in
    specialised LISP hardware in Japan, in the United States and in
    Europe. The subject matter of each of these three sections is so
    tightly interwoven however that the detailed discussion of some
    material may be postponed until a later section although some
    readers it might seem appropriate for inclusion earlier. It shoud
    also be mentioned that this is a survey, therefore the items have
    been selected for mention on the grounds of interest rather than
    completeness.",
  paper = "Padg85.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Kreisel, G.}
\begin{chunk}{axiom.bib}
@inproceedings{Krei85,
  author = "Kreisel, G.",
  title = {{Proof Theory and the Synthesis of Programs: Potential and
            Limitations}},
  booktitle = "EUROCAL 85",
  publisher = "Springer"
  paper = "Krei85.pdf"
}

\end{chunk}

\index{Vazou, Niki}
\index{Breitner, Joachim}
\index{Kunkel, Rose}
\index{Horn, David Van}
\index{Hutton, Graham}
\begin{chunk}{axiom.bib}
@inproceedings{Vazo18,
  author = "Vazou, Niki and Breitner, Joachim and Kunkel, Rose and
            Horn, David Van and Hutton, Graham",
  title = {{Theorem Proving for All: Equational Reasoning in Liquid
            Haskell}},
  booktitle = "Haskell'18",
  publisher = "ACM",
  year = "2018",
  isbn = "978-1-4503-5835-4",
  link = "\url{http://www.cs.nott.ac.uk/~pszgmh/tpfa.pdf}",
  abstract =
    "Equational reasoning is one of the key features of pure
    functional languages such as Haskell. To date, however, such
    reasoning always took place eternally to Haskell, either manually
    on paper, or machanised in a theorem prover. This article shows
    how equational reasoning can be performed directly and seamlessly
    within Haskell itself, and be checked using Liquid Haskell. In
    particular, language learners -- to whom external theorem provers
    are out of reach -- can benefit from having their proofs
    mechanically checked. Concretely, we show how the equational
    proofs and derivations from Hutton's textbook can be recast as
    proofs in Haskell (spoiler: they look essentially the same).",
  paper = "Vazo18.pdf"
}

\end{chunk}

\index{Jaskelioff, Mauro}
\index{Ghani, Neil}
\index{Hutton, Graham}
\begin{chunk}{axiom.bib}
@inproceedings{Jask08,
  author = "Jaskelioff, Mauro and Ghani, Neil and Hutton, Graham",
  title = {{Modularity and Implementation of Mathematical Operational
            Semantics}},
  publisher = "Elsevier",
  year = "2008",
  link = "\url{http://www.cs.nott.ac.uk/~pszgmh/modular.pdf}",
  abstract =
    "Structural operational semantics is a popular technique for
    specifying the meaning of programs by means of inductive
    clauses. One seeks syntactic restrictions on those clauses so that
    the resulting operational semantics is well-behaved. This approach
    is simple and concrete but it has some drawbacks. Turi pioneered a
    more abstract categorical treatment based upon the idea that
    operational semantics is essentially a distribution of syntax over
    behaviour. In this article we take Turi's approach in two new
    directions. Firstly, we show how to write operational semantics as
    modular components and how to combine such components to specify
    complete languages. Secondly, we show how the categorical nature
    of Turi's operational semantics makes it ideal for implementation
    in a functional programming language such as Haskell.",
  paper = "Jask08.pdf"
}

\end{chunk}

\index{Pickard, Mitchell}
\index{Hutton, Graham}
\begin{chunk}{axiom.bib}
@inproceedings{Pick20,
  author = "Pickard, Mitchell and Hutton, Graham",
  title = {{Dependently-Typed Compilers Don't Go Wrong}},
  booktitle = "Programming Languages",
  publisher = "ACM",
  year = "2020",
  link = "\url{http://www.cs.nott.ac.uk/~pszgmh/well-typed.pdf}",
  abstract =
    "Compilers are difficult to write, and difficult to get
    right. Bahr and Hutton recently developed a new technique for
    calculating compilers directly from specifications of their
    correctness, which ensures that the resulting compilers are
    correct-by-construction. To date, however, this technique has only
    been applicable to source languages that are untyped. In this
    article, we show that moving to a dependently-typed setting allows
    us to naturally support typed source languages, ensure that all
    compilation components are type-safe, and make the resulting
    calculations easier to mechanically check using a proof
    assistant.",
  paper = "Pick20.pdf"
}

\end{chunk}

\index{Doorn, Floris van}
\index{Ebner, Gabriel}
\index{Lewis, Robert Y.}
\begin{chunk}{axiom.bib}
@misc{Door20,
  author = "Doorn, Floris van and Ebner, Gabriel and 
            Lewis, Robert Y.",
  title = {{Maintaining a Library of Formal Mathematics}},
  link = "\url{https://arxiv.org/pdf/2004.03673}",
  year = "2020",
  abstract =
    "The Lean mathematical library mathlib is developed by a community
    of users with very different backgrounds and levels of
    experience. To lower the barrier of entry for contributors and to
    lessen the burden of reviewing contributions, we have developed a
    number of tools for the library which check proof developments for
    subtle mistakes in the code and generate documentation suited for
    our varied audience.",
  paper = "Door20.pdf"
}

\end{chunk}

\index{Rekdal, Ole Bjorn}
\begin{chunk}{axiom.bib}
@article{Rekd14,
  author = "Rekdal, Ole Bjorn",
  title = {{Academic Urban Legends}},
  journal = "Social Studies of Science",
  volume = "44",
  number = "4",
  pages = "638-654",
  year = "2014",
  abstract =
    "Many of the messages presented in respectable scientific
    publications are, in fact, based on various forms of rumors. Some
    of these rumors appear so frequently, and in such complex,
    colorful, and entertaining ways that we can think of them as
    academic urban legends. The explanation for this phenomenon is
    usually that the authors have lazily, sloppily, or fraudulently
    employed sources, and peer reviewers and editors have not
    discovered these weaknesses in the manuscripts during
    evaluation. To illustrate this phenomenon, I draw upon a
    remarkable case in which a decimal point error appears to have
    misled millions into believing that spinach is a good nutritional
    source of iron. Through this example, I demonstrate how an
    academic urban legend can be conceived and born, and can continue
    to grow and reproduce within academia and beyond.",
  paper = "Rekd14.pdf",
  keywords = "DONE"
}

\end{chunk}

\index{Hoare, C.A.R}
\begin{chunk}{axiom.bib}
@article{Hoar72a,
  author = "Hoare, C.A.R",
  title = {{Proof of Correctness of Data Representations}},
  journal = "Acta Informatica",
  volume = "1",
  pages = "271-281",
  year = "1972",
  abstract =
    "A powerful method of simplifying the proofs of program
    correctness is suggested; and some new light is shed on the
    problem of functions with side-effects.",
  paper = "Hoar72a.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Streicher, Thomas}
\begin{chunk}{axiom.bib}
@book{Stre91,
  author = "Streicher, Thomas",
  title = {{Semantics of Type Theory}},
  year = "1991",
  publisher = "Springer",
  isbn = "978-1-4612-6757-7"
}

\end{chunk}

\index{Dijkstra, E.W.}
\begin{chunk}{axiom.bib}
@misc{Dijk67,
  author = "Dijkstra, E.W.",
  title = {{A Constructive Approach to the Problem of Program Correctness}},
  year = "1967",
  link = "\url{https://www.cs.utexas.edu/users/EWD/ewd02x/EWD209.PDF}",
  abstract =
    "As an alternative to methods by which the correctness of given
    programs can be established a posteriori, this paper proposes to
    control the process of program generation such as to produce a
    priori correct programs. An example is treated to show the form
    that such a control then might take. This exampe comes from the
    field of parallel programming; the way in which it is treated is
    representative for the way in which a whole multiprogramming
    system has actually been constructed.",
  paper = "Dijk67.pdf",
  keywords = "printed"
}

\end{chunk}

\index{Schoett, Oliver}
\begin{chunk}{axiom.bib}
@article{Scho90,
  author = "Schoett, Oliver",
  title = {{Behavioural Correctness of Data Representations}},
  journal = "Science of Computer Programming",
  volume = "14",
  year = "1990",
  pages = "43-57",
  abstract =
    "Two methods for proving the correctness of data representation
    are presented which employ a mathematical relation between the
    data values in a representation and those in its abstract
    model. One method reflects the behavioural equaivalence relation
    of abstract data type theory, and the other a new ``behavioural
    inclusion'' notion that formalizes the idea of a ``partial
    representation'' of a data type.

    These correctness concepts and proof methods are strictly more
    general than the conventional ones based on abstraction functions,
    and they are no longer affected by ``implementation bias'' in
    specifications.", 
  paper = "Scho90.pdf",
  keywords = "printed"
}  

\end{chunk}

\index{Macor, Jackson}
\begin{chunk}{axiom.bib}
@misc{Maco15,
  author = "Macor, Jackson",
  title = {{A Brief Introduction to Type Theory and the Univalence
            Axiom}},
  year = "2015",
  comment = "U. Chicago REU",
  link = "\url{https://math.uchicago.edu/~may/REU2015/REUPapers/Macor.pdf}",
  abstract =
    "In this paper, we will introduce the basic concepts and notation
    of modern type theory in an informal manner. We will discuss
    functions, type formation, the nature of proof, and proceed to
    prove some basic results in sentential logic. These efforts will
    culminate in a proof of the axiom of choice in type theory. We
    will further introduce a few concepts in homotopy type theory, a
    modern invention which sees to provide a foundation of mathematics
    without ZFC type theory. We will conclude with the univalence
    axiom, an indispensable tool in homotopy type theory, and use it
    to prove a stronger version of the axiom of choice.",
  paper = "Maco15.pdf",
  keywords = "printed, DONE"
}

\end{chunk}

\index{Hebisch, Waldemar}
\begin{chunk}{axiom.bib}
@misc{Hebi20,
  author = "Hebisch, Waldemar",
  title = {{History of FriCAS}},
  year = "2020",
  link = "\url{http://fricas.sourceforge.net/history.html}",
  keywords = "axiomref, DONE"
}

\end{chunk}

\index{Kaltofen, Erich}
\index{Rolletschek, Heinrich}
\begin{chunk}{axiom.bib}
@inproceedings{Kalt85f,
  author = "Kaltofen, Erich and Rolletschek, Heinrich",
  title = {{Arithmetic in Quadratic Fields with Unique Factorization}},
  booktitle = "Research Contributions from the Euro. Conf. on Comp. Alg.",
  series = "Lecture Notes in Computer Science Volume 204",
  volume = "2",
  pages = "279-288",
  year = "1985",
  isbn = "0-387-15983-5 (vol. 1),0-387-15984-3 (vol. 2)",
  paper = "Kalt85f.pdf"
}

\end{chunk}

\index{Kaltofen, Erich}
\index{Rolletschek, Heinrich}
\begin{chunk}{axiom.bib}
@inproceedings{Kalt85f,
  author = "Kaltofen, Erich and Rolletschek, Heinrich",
  title = {{Arithmetic in Quadratic Fields with Unique Factorization}},
  booktitle = "Research Contributions from the Euro. Conf. on Comp. Alg.",
  series = "Lecture Notes in Computer Science Volume 204",
  volume = "2",
  pages = "279-288",
  year = "1985",
  isbn = "0-387-15983-5 (vol. 1),0-387-15984-3 (vol. 2)",
  abstract =
    "In a quadratic field $\mathbb{Q}(\sqrt{D})$, $D$ a squarefree
    integer, with class number 1 any algebraic integer can be
    decomposed uniquely into primes but for only 21 domains Euclidean
    algorithms are known. We prove that for $D \le -19$ even remainder
    sequences with possibly non-decreasing norms cannot determine the
    GCD of arbitrary inputs. We then show how to compute the greatest
    common divisor of the algebraic integers in any fixed
    $\mathbb{Q}(\sqrt{D})$ with class number 1 in $O(S^2)$ binary
    steps where $S$ is the number of bits needed to encode the
    inputs. We also prove that in any domain the computation of the
    prime factorization of an algebraic integer can be reduced in
    polynomial-time to factoring its norm into rational primes. Our
    reduction is based on a constructive version of a theorem by
    A. Thue. Finally we present another GCD algorithm for complex
    quadratic fields based on a short lattice vector construction.",
  paper = "Kalt85f.pdf"
}

\end{chunk}

\index{Greif, J.M.}
\begin{chunk}{axiom.bib}
@inproceedings{Grei85,
  author = "Greif, J.M.",
  title = {{The SMP Pattern Matcher}},
  booktitle = "Research Contributions from the Euro. Conf. on Comp. Alg.",
  series = "Lecture Notes in Computer Science Volume 204",
  volume = "2",
  pages = "303-314",
  year = "1985",
  isbn = "0-387-15983-5 (vol. 1),0-387-15984-3 (vol. 2)",
  abstract =
    "A new pattern matcher has been implemented for the symbolic
    manipulation program SMP. The pattern matcher correctly treats
    associative and commutative functions, functions with arbitrary
    symmetries under permutations of their arguments, and patterns
    subject to arbitrary logical predicates. A pattern is said to match
    a candidate instance if the set of expressions it represents is a
    superset of those represented by the instance (either or both may
    contain generic symbols which match possibly infinite sets of
    expressions). The matcher is primarily syntactic, but is able to
    find matches for syntactically different expressions which become
    literally equivalent upon replacing the generic symbols in the
    pattern by their bindings to subexpressions of the instance and
    simplifying the result. The bindings must be determined by literal
    comparison with the instance, not by solving equations.

    This paper discusses issues arising during the construction of
    this pattern matcher, and gives examples of using pattern matching
    for extending the power of built-in operations and adding new
    mathematical knowledge to a symbol manipulation program.",
  paper = "Grei85.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Marti, Jed}
\begin{chunk}{axiom.bib}
@article{Mart83,
  author = "Marti, Jed",
  title = {{The Little META Translator Writing System}},
  journal = "Software -- Practice and Experience",
  volume = "13",
  pages = "941-959",
  year = "1983",
  abstract =
    "The Little META Translator Writing System is a tool for
    implementing experimental compilers, interpreters, preprocessors
    and higher level translator writing systems on a
    microprocessor. This paper presents the salient features of the
    system through the implementation of three translators. Some
    knowledge of LISP is assumed.",
  paper = "Mart83.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Fateman, Richard J.}
\begin{chunk}{axiom.bib}
@misc{Fate96a,
  author = "Fateman, Richard J.",
  title = {{What Computer Algebra Systems Can't Solve Simple
            Equations}},
  year = "1996",
  link = "\url{https://people.eecs.berkeley.edu/~fateman/papers/y=z2w.pdf}",
  paper = "Fate96a.pdf"
}

\end{chunk}

\index{Paule, Peter}
\begin{chunk}{axiom.bib}
@book{Paul13,
  author = "Paule, Peter",
  title = {{Mathematics, Computer Science and Logic -- A Never Ending
            Story}}, 
  publisher = "Springer",
  year = "2013",
  paper = "Paul13.pdf"
}

\end{chunk}

\index{Buchberger, B.}
\index{Loos, R.}
\begin{chunk}{axiom.bib}
@inbook{Buch82a
  author = "Buchberger, Bruno and Loos, Rudiger",
  title = {{Algebraic Simplification}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "11-44",
  abstract =
    "Some basic techniques for the simplification of terms are
    surveyed. In two introductory sections the problem of canonical
    algebraic simplification is formally stated and some elementary
    facts are derived that explain the fundamental role of
    simplification in computer algebra. In the subsequent sections two
    major groups of simplification techniques are presented: special
    techniques for simplifying terms over numerical domains and
    completion algorithms for simplification with respect to sets of
    equations. Within the first group canonical simplification
    algorithms for polynomials, rational expressions, radical
    expressions and transcendental expressions are treated (Sections
    3-7). As examples for completion algorithms the Knuth-Bendix
    algorithm for rewrite rules and an algorithm for completing bases
    of polynomial ideals are described (Sections 8-11).",
  paper = "Buch82.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Lauer, M.}
\begin{chunk}{axiom.bib}
@InCollection{Laue82,
  author = "Lauer, M.",
  title = {{Computing by Homomorphic Images}},
  booktitle = "Computer Algebra: Symbolic and Algebraic Computation",
  pages = "139-168",
  year = "1982",
  publisher = "Springer",
  isbn = "978-3-211-81684-4",
  abstract =
    "After explaining the general technique of Computing by homomorphic
    images, the Chinese remainder algorithm and the Hensel lifting
    construction are treated extensively. Chinese remaindering is first
    presented in an abstract setting. Then the specialization to Euclidean
    domains, in particular $\mathbb{Z}$, $\mathbb{K}[y]$, and
    $\mathbb{Z}[y_1,\ldots,y_n]$ is treated. The lifting construction
    is first also presented in an abstract form from which Hensel's
    Lemma derives by specialization. After introducing Zassenhaus'
    quadratic lifting construction, again, the case of $\mathbb{Z}$
    and $\mathbb{X}[y_1,\ldots,y_r]$ is considered. For both techniques,
    Chinese remaindering as well as the lifting algorithms, a complete
    computational example is presented and the most frequent application
    is discussed."
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Loos, R.}
\begin{chunk}{axiom.bib}
@inbook{Loos82,
  author = "Loos, R",
  title = {{Generalized Polynomial Remainder Sequences}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "115-138",
  abstract =
    "Given two polynomials over an integral domain, the problem is to
    compute their polynomial remainder sequence (p.r.s) over the same
    domain. Following Habischt, we show how certain powers of leading
    coefficients enter systematically all following remainders. The
    key tool is the subresultant chain of two polynomials. We study
    the primitive, the reduced and the improved subresultant p.r.s
    algorithm of Brown and Collins as basis for computing polynomial
    greatest common divisors, result and or Sturm sequences. Habicht's
    subresultant theorem allows new and simple proofs of many results
    and algorithms found in different ways in computer algebra.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Neubuser, J.}
\begin{chunk}{axiom.bib}
@inbook{Neub82,
  author = "Neubuser, J.",
  title = {{Computing with Groups and Their Character Tables}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "45-56",
  abstract =
    "In this survey an attempt is made to give some impression of the
    capabilities of currently available programs for computations with
    finitely generated groups and their representations.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Norman, A.C.}
\begin{chunk}{axiom.bib}
@inbook{Norm82a,
  author = "Norman, A.C.",
  title = {{Integration in Finite Terms}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "57-70",
  abstract =
    "A survey on algorithms for integration in finite terms is
    given. The emphasis is on indefinite integration. Systematic
    methods for rational, algebraic and elementary transcendental
    integrands are reviewed. Heuristic techniques for indefinite
    integration, and techniques for definite integration and ordinary
    differental equations are touched only briefly.",
  paper = "Buch82.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Norman, A.C.}
\begin{chunk}{axiom.bib}
@inbook{Norm82b,
  author = "Norman, A.C.",
  title = {{Computing in Transcendental Extensions}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "169-172",
  abstract =
    "Performing the rational operations in a field extended by a
    transcendental element is equivalent to performing arithmetic in
    the field of rational functions over the field. The computational
    difficulty associated with such extensions is in verifying that
    proposed extensions are transcendental. When the extensions being
    considered are functions, and where a differentiation operator can
    be defined for them, structure theorems can be used to determine
    the character of the extension and to exhibit a relationship
    between the adjoined element and existing quantities in case the
    adjoined element is not transcendental.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Lafon, J.C.}
\begin{chunk}{axiom.bib}
@inbook{Lafo82,
  author = "Lafon, J.C.",
  title = {{Summation in Finite Terms}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "71-77",
  abstract =
    A survey on algorithms for summation in finite terms is given. After a
    precise definition of the problem the cases of polynomial and rational
    summands are treated. The main concern of this paper is a description
    of Gosper's algorithm, which is applicable for a wide class of
    summands.  Karr's theory of extension difference fields and some
    heuristic techniques are touched on briefly.",
  paper = "Buch82.pdf",
  keywords = "axiomref, survey"
}

\end{chunk}

\index{Collins, G.E.}
\index{Mignotte, M.}
\index{Winkler, F.}
\begin{chunk}{axiom.bib}
@inbook{Coll82,
  author = "Collins, G.E. and Mignotte, M. and Winkler, F.",
  title = {{Arithmetic in Basic Algebraic Domains}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  isbn = "978-3-211-81684-4",
  pages = "189-220",
  year = "1982",
  abstract =
    "This chapter is devoted to the arithmetic operations, essentially
    addition, multiplication, exponentiation, division, gcd calculations
    and evaluation, on the basic algebraic domains. The algorithms for
    these basic domains are those most frequently used in any computer
    algebra system. Therefore the best known algorithms, from a
    computational point of view, are presented. The basic domains
    considered here are the rational integers, the rational numbers,
    integers modulo $m$, Gaussian integers, polynomials, rational
    functions, power series, finite fields and $p$-adic numbers. BOunds on
    the maximum, minimum and average computing time ($t^{+},t^{-},t^{*}$) for
    the various algorithms are given.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Collins, G.E.}
\begin{chunk}{axiom.bib}
@inbook{Coll82b,
  author = "Collins, G.E.",
  title = {{Quantifier Elimination for Real Closed Fields: A Guide to
            the Literature}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "79-82",
  abstract =
    "This article provides a brief summary of the most important
    publications relating to quantifier elimination for the elementary
    theory of real closed fields. Especially mentioned is the
    cylindrical algebraic decomposition method and its relation to the
    facilities of computer algebra facilities.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Collins, G.E.}
\index{Loos, R.}
\begin{chunk}{axiom.bib}
@inbook{Coll82c,
  author = "Collins, G.E. and Loos, R.",
  title = {{Real Zeros of Polynomials}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "83-94",
  abstract =
    "Let $A$ be a polynomial over $\mathbb{Z}$, $\mathbb{Q}$,
    $\mathbb{Q}(\alpha)$ where $\alpha$ is a real algebraic
    number. The problem is to compute a sequence of disjoint intervals
    with rational endpoints, each containing exactly one real zero of
    $A$ and together containing all real zeros of $A$. We describe an
    algorithm due to Kronecker based on the minimum root separation,
    Sturm's algorithm, an algorithm based on Rolle's theorem due to
    Collins and Loos and the modified Uspensky algorithm due to
    Collins and Aritas. For the last algorithm a recursive version
    with correctness proof is given which appears in print for the
    first time.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Loos, R.}
\begin{chunk}{axiom.bib}
@inbook{Loos82a,
  author = "Loos, R",
  title = {{Computing in Algebraic Extensions}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "173-188",
  abstract =
    "The aim of this chapter is an introduction to elementary
    algorithms in algebraic extensions, mainly over $\mathbb{Q}$ and,
    to some extent, over $GF(p)$. We will talk about arithmetic in
    $\mathbb{Q}(\alpha)$ and $GF(p^n)$ in Section 1 and some
    polynomial algorithms with coefficients from these domains in
    Section 2. Then, we will consider the field $K$ of all algebraic
    numbers over $\mathbb{Q}$ and show constructively that $K$ indeed
    is a field that multiple extensions can be replaced by single ones
    and the $K$ is algebraically closed, i.e. that zeros of algebraic
    number polynomials will be elements of $K$ (Section 4-6). For this
    purpose we develop a simple resultant calculus which reduces all
    operations on algebraic numbers to polynomial arithmetic on long
    integers together with some auxiliary arithmetic on rational
    intervals (Secion 3). Finally, we present some auxiliary algebraic
    number algorithms used in other chapters of this volume (Section
    7). This chapter does not include any special algorithms of
    algebraic number theory. For an introduction and survey with an
    extensive bibliography the reader is referred to Zimmer.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Hulzen, J.A. van}
\index{Calmet, J.}
\begin{chunk}{axiom.bib}
@inbook{Hulz82a,
  author = "Hulzen, J.A. van and Calmet, J.",
  title = {{Computer Algebra Systems}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "221-244",
  abstract =
    A survey is given of computer algebra systems, with emphasis on
    design and implementation aspects, by presenting a review of the
    development of ideas and methods in a historical perspective, by
    us considered as instrumental for a better understanding of the
    rich diversity of now available facilities. We first indicate
    which classes of mathematical expressions can be stated and
    manipulated in different systms before we touch on different
    general aspects of usage, design and implementation, such as
    language design, encoding, dynamic storage allocation and a
    symbolic-numeric interface. Then we discuss polynomial and
    rational function systems, by describing ALTRAN and SAC-2. This is
    followed by a comparison of some of the features of MATHLAB-68,
    SYMBAL and FORMAC, which are pretended general purpose
    systems. Before considering giants (MACSYMA and SCRATCHPAD) and
    gnomes (muMATH-79), we give the main characteristics of TRIGMAN,
    CAMAL and REDUCE, systems we tend to consider as grown out special
    purpose facilities. Finally we mention some modern algebra systems
    (CAYLEY and CAMAC-79) in relation to recent proposals for a
    language for computational algebra. We conclude by stipulating the
    importance of documentation. Throughout this discussion related
    systems and facilities will be mentioned. Noticeable are ALKAHEST
    II, ALPAK, ANALITIK, ASHMEDAI, NETFORM, PM, SAC-1, SCHOONSCHIP,
    SHEEP, SML, SYCOPHANTE and TAYLOR.",
  paper = "Buch82.pdf",
  keywords = "axiomref"
}

\end{chunk}

\index{Calmet, J.}
\index{Hulzen, J.A. van}
\begin{chunk}{axiom.bib}
@inbook{Calm82,
  author = "Calmet, J. and Hulzen, J.A. van",
  title = {{Computer Algebra Applications}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "245-258",
  abstract =
    "A survey of applications based either on fundamental algorithms
    in computer algebra or on the use of a computer algebra system is
    presented. Since many survey articles are previously published, we
    did not attempts to be exhaustive. We discuss mainly recent work
    in biology, chemistry, physics, mathematics and computer science,
    thus again confirming that applications have both engineering and
    scientific aspects, i.e. apart from delivering results they assist
    in gaining insight as well.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Mignotte, M.}
\begin{chunk}{axiom.bib}
@inbook{Mign82,
  author = "Mignotte, M.",
  title = {{Some Useful Bounds}},
  booktitle = {{Computer Algebra: Symbolic and Algebraic Computation}},
  publisher = "Springer",
  year = "1982",
  isbn = "978-3-211-81684-4",
  pages = "259-264",
  abstract =
    "Some fundamental inequalities for the following values are
    listed: the determinant of a matrix, the absolute value of the
    roots of a polynomial, the coefficients of divisors of
    polynomials, and the minimal distance between the roots of a
    polynomial. These inequalities are useful for the analysis of
    algorithms in various areas of computer algebra.",
  paper = "Buch82.pdf"
}

\end{chunk}

\index{Moller, Anders}
\index{Schwartzbach, Michael I.}
\begin{chunk}{axiom.bib}
@book{Moll19,
  author = "Moller, Anders and Schwartzbach, Michael I.",
  title = {{Static Program Analysis}},
  publisher = "Aarhus University",
  year = "2019"
}

\end{chunk}

\index{Guillaume, Alexandre}
\begin{chunk}{axiom.bib}
@phdthesis{Guil98,
  author = "Guillaume, Alexandre",
  title = {{De ALDOR a Zermelo}},
  school = "University Paris VI",
  year = "1998"
}

\end{chunk}

